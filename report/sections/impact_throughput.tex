\subsection{Test introduction}

    The throughput is probably the most relevant index of the network
    quality. In the context of \emph{mesh networking} a natural concern is
    how the overhead coming from the underlaying routing protocol affects
    it.

    As we previously mentioned, both the protocols we worked
    with are of the \emph{pro-active} family: they periodically sends
    packets which are used to build and maintain the topology. Obviously
    the number of control packets generated by the routing protocol grows
    with the number of nodes which are part of the network thus, since our
    testbed was composed by just a few hosts, what we reasonably expected
    was a minimal impact on the overall performances.

    The core of the experiment consisted in a simple throughput test with
    \netperf\ on three different network topologies. For both the
    analyzed mesh protocols we performed the test while the protocol's
    software was running. We also run an instance of the test with
    statically compiled routes in order to obtain an \emph{ideal overhead
    situation} to be used as term of comparison.

    For each combination of network topology and routing protocol, we
    executed 10 consecutive \netperf\ measurement, each one delayed by 10 seconds
    from the previous. Each run lasted 60 seconds and was performed by
    running the following command on the \emph{source} machine (10.0.0.65):
\begin{verbatim}
netperf -H 10.0.0.67 -D 1 -l 60
\end{verbatim}

    In order to keep coherence between tests, we always used the host
    10.0.0.67 as target.

    \subsubsection{Topology}

        The \emph{direct link} topology is the simplest. Only two laptops
        have been enabled (see Figure~\ref{pic:LayoutDirect}):
        \begin{itemize}
        \item   The \emph{Source} node (address 10.0.0.65);
        \item   The \emph{Sink} node (address 10.0.0.67).
        \end{itemize}

        \Picture{images/direct}
                {.49\columnwidth}
                {Configuration with single direct link}
                {pic:LayoutDirect}

    \subsubsection{Results}

        In this situation the message exchange is very limited: every
        synchronization step consists in the two hosts simply exchanging a
        few UDP packets (as described in Section~\ref{sec:Intro}). The
        network performances measure
        confirms our hypothesis: as shown in Table~\ref{tab:ThrDirect} the
        performance variation is so minimal that is more likely to be
        caused by the different condition of the wireless channel
        among experiments, rather then the actual overhead of the
        protocols.

        \Picture{images/throughput_plot_direct}
                {0.7 \columnwidth}
                {Impact of the meshing protocols on the throughput of the
                 direct link topology. The three boxplots show,
                 respectively, the performance with \emph{static routes},
                 \batman\ and \olsr.}
                {pic:ThpDirect}

        \begin{table}[htbp]
            \centering
            \begin{tabular}{rcccccccc}
            \toprule
            Protocol & Average & Variance & Min & 1st Quartile &
            Median & 3rd Quartile & Max & Comp. w.r.t.\\
            & \footnotesize{\MBitsSec} &
            & \footnotesize{\MBitsSec} &
              \footnotesize{\MBitsSec} &
              \footnotesize{\MBitsSec} &
              \footnotesize{\MBitsSec} &
              \footnotesize{\MBitsSec} & Static\\

            \midrule
            Static      & 16.970 & 0.191 & 15.74 & 16.67 & 16.97 & 17.26
                        & 18.08  & - \\
            \batman\    & 16.926 & 0.251 & 15.41 & 16.58 & 16.94 & 17.3
                        & 18.4   & 0.997 \\
            \olsr\      & 16.990 & 0.313 & 14.98 & 16.65 & 17.02 & 17.35
                        & 18.86  & 1.001 \\
            \bottomrule
            \end{tabular}
            \caption{Throughput result for direct topology. The last
                     column shows a proportional comparison with the
                     average throughput of the statically compiled
                     routes.}
            \label{tab:ThrDirect}
        \end{table}

\subsection{Test with 1 hop}

    \subsubsection{Topology}

        The \emph{1 hop} topology is composed by 3 computer arranged
        in a chain (see Figure~\ref{pic:Layout1Hop}):
        \begin{itemize}
        \item   The \emph{Source} node (address 10.0.0.65);
        \item   The node acting as bridge (address 10.0.0.66);
        \item   The \emph{Sink} node (address 10.0.0.67).
        \end{itemize}

        \Picture{images/1hop}
                {.90\columnwidth}
                {Configuration with 1 hop}
                {pic:Layout1Hop}

        To implement this scheme we needed to prevent the \emph{Source}
        node from communicating directly with the \emph{Sink} one and vice
        versa, but since we were operating with a wireless medium it
        wasn't possible to physically stop the 2 computer from seeing each
        other. As workaround we exploited \emph{iptables} to filter out
        incoming packet based on the MAC address of the sender.

        \begin{itemize}
        \item On \emph{Source}:
            \begin{verbatim}
iptables -A INPUT -m mac --mac-source $(sinkMAC) -j DROP;
            \end{verbatim}

        \item On \emph{sink}:
            \begin{verbatim}
iptables -A INPUT -m mac --mac-source $(sourceMAC) -j DROP;
            \end{verbatim}

        \end{itemize}

    \subsubsection{Results}

        As before it was reasonable to assume the overhead imposed by
        the routing protocols to be negligible, as the number of node was
        really small: again we expected to measure equivalent average
        throughputs. Instead the results we obtained show otherwise.
        As we can clearly see in  Figure~\ref{pic:Thp1Hop} and
        Table~\ref{tab:Thr1Hop}, when using \batman\ and \olsr\, the average
        throughput decreases significantly while the variance increases notably.

        In our opinion this result does not provide a truthful
        picture of the reality. This consideration is sustained by
        the fact that we have more overhead than in a 2 hops configuration
        (see Paragraph~\ref{subsec:2hop}), in which intuitively the
        interference should be much more detectable. At any rate, we report
        the obtained experimental data.

        \Picture{images/throughput_plot_1hop}
                {0.7 \columnwidth}
                {Impact of the meshing protocols on the throughput of the
                 1-hop topology. The three boxplots show, respectively, the
                 performance with \emph{static routes}, \emph{\batman} and
                 \emph{\olsr}}
                {pic:Thp1Hop}

        \begin{table}[htbp]
            \centering
            \begin{tabular}{rcccccccc}
            \toprule
            Route & Average & Variance & Min & 1st Quartile &
            Median & 3rd Quartile & Max & Comp. w.r.t.\\
            & \footnotesize{\MBitsSec} & & \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} &
            \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} & Static\\
            \midrule
            Static      & 8.496 & 1.118 & 2.635 & 8.09 & 8.62 & 9.08
                        & 11.12 & - \\
            \batman\    & 7.973 & 2.197 & 1.455 & 7.425 & 8.27 & 8.855
                        & 11.71 & 0.938 \\
            \olsr\      & 6.924 & 7.68 & 0.096 & 6.91 & 8.01 & 8.62
                        & 10.21 & 0.815 \\
            \bottomrule
            \end{tabular}
            \caption{Throughput result for 1hop topology. The last
                     column shows a proportional comparison with the
                     average throughput of the statically compiled
                     routes.}
            \label{tab:Thr1Hop}
        \end{table}

\subsection{Test with 2 hop} \label{subsec:2hop}

    \subsubsection{Topology}

        The \emph{2 hops} topology is composed by 4 computers disposed
        in a chain (see Figure~\ref{pic:Layout2Hop}):
        \begin{itemize}
        \item   The \emph{Source} node (address 10.0.0.65);
        \item   The node acting as first hop (address 10.0.0.66);
        \item   The node acting as second hop (address 10.0.0.68);
        \item   The \emph{Sink} node (address 10.0.0.67).
        \end{itemize}

        \Picture{images/2hop}
                {.90\columnwidth}
                {Configuration with 2 hop}
                {pic:Layout2Hop}

        \noindent The rules for \emph{iptables} we used to implement this
        configuration are the following:

        \begin{itemize}
        \item On the \emph{Source} node:
        \begin{verbatim}
iptables -A INPUT -m mac --mac-source $(sinkMAC) -j DROP;
iptables -A INPUT -m mac --mac-source $(hop2MAC) -j DROP;
        \end{verbatim}

        \item On the first hop:
        \begin{verbatim}
iptables -A INPUT -m mac --mac-source $(sinkMAC) -j DROP;
        \end{verbatim}

        \item On the second hop:
        \begin{verbatim}
iptables -A INPUT -m mac --mac-source $(sourceMAC) -j DROP;
        \end{verbatim}

        \item On the \emph{Sink}
        \begin{verbatim}
iptables -A INPUT -m mac --mac-source $(sourceMAC) -j DROP;
iptables -A INPUT -m mac --mac-source $(hop1MAC) -j DROP;
        \end{verbatim}

        \end{itemize}

    \subsubsection{Results}

        This time the results are more compatible with our hypothesis.
        As in the direct link topology the throughput difference between
        the three measurements is negligible and most likely caused by
        noise on the wireless channel.

        \Picture{images/throughput_plot_2hop}
                {0.7 \columnwidth}
                {Impact of the meshing protocols on the throughput of the
                 2-hops topology. The three boxplots show, respectively,
                 the performance with \emph{static routes}, \emph{\batman}
                 and \emph{\olsr}}
                {pic:Thp2Hops}

        \begin{table}[htbp]
            \centering
            \begin{tabular}{rcccccccc}
            \toprule
            Route & Average & Variance & Min & 1st Quartile &
            Median & 3rd Quartile & Max & Comp. w.r.t.\\
            & \footnotesize{\MBitsSec} & & \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} &
            \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} & Static\\
            \midrule
            Static      & 5.344 & 2.183 & 1.332 & 4.82 & 5.84 & 6.27
                        & 8.22 & - \\
            \batman\    & 5.381 & 2.03 & 0.753 & 4.92 & 5.84 & 6.29
                        & 8.81 & 1.007 \\
            \olsr\      & 5.193 & 3.228 & 0.12 & 5.015 & 5.82 & 6.33
                        & 7.77 & 0.972 \\
            \bottomrule
            \end{tabular}
            \caption{Throughput result for 2hop topology. The last
                     column shows a proportional comparison with the
                     average throughput of the statically compiled
                     routes.}
            \label{tab:Thr2Hop}
        \end{table}

\subsection{Considerations about Throughput}

    The results shown in this section tend to support our initial
    hypothesis: the overhead induced by the routing protocols doesn't
    affect the throughput performances of the network. This is due to the
    limited number of node we were able to employ.

    We would like to point out a peculiar
    aspect we seen while performing the tests when using \olsr: we
    observed multiple \netperf\ iterations in which the throughput fell
    to 0 for some seconds and then recovered.
    Listing~\ref{lst:netperf-olsr-recover} shows how in an experiment the
    communication executed a step of 4.97 seconds where, on average, each
    step lasts 1.19 seconds.

    In other executions, like the one showed in
    Listing~\ref{lst:netperf-olsr-zero}, the throughput went to 0 and
    remained flat for a remarkable time (in this case 25.16 seconds).
    We observed some slight fluctuations both while using \batman\ and
    \emph{static routes} as well, however the degradation was never so
    dramatic.

    The amount of data we collected on this issue however is not
    nearly sufficient to be statistically meaningful: we cannot say for sure if this
    is to be attributed to the routing protocol or to \netperf\ (which
    proved itself not so stable in other situations). Anyhow we
    think this fact is worth mentioning and could be purposely analyzed
    in the future.

    \begin{figure}[bthp]
    \begin{lstlisting}
    Interim result:    4.84 10^6bits/s over 1.43 seconds
    Interim result:    1.61 10^6bits/s over 4.97 seconds
    Interim result:    7.29 10^6bits/s over 1.08 seconds
    \end{lstlisting}
    \caption{Example of odd behavior while using \olsr: throughput falls
             to 0 and recovers.}
    \label{lst:netperf-olsr-recover}
    \end{figure}

    \begin{figure}[bthp]
    \begin{lstlisting}
    Interim result:    8.29 10^6bits/s over 1.26 seconds
    Interim result:    0.33 10^6bits/s over 25.16 seconds
    \end{lstlisting}
    \caption{Example of odd behavior while using \olsr: throughput falls
             to 0 and recovers after a long time.}
    \label{lst:netperf-olsr-zero}
    \end{figure}
