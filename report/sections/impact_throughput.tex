\subsection{Test introduction}

    The throughput is probably the most relevant index of the network
    quality. In the context of \emph{mesh networking} a natural concern
    is how the overhead coming from the underlaying routing protocol
    affects it.

    As we previously mentioned, both the protocols we worked
    with are of the \emph{proactive} family: they periodically sends
    packets which are used to build tand maintain he topology. The
    more node are present in the network, the more control packet will
    be generated by the routing protocol.
    Since we didn't have many laptops available, our testbed was composed
    by just a few hosts: a reasonable expectation is that the overhead
    induced by the routing protocol doesn't impact the overall
    performances.

    The core of the experiment consisted in a simple throughput test with
    \netperf\ on three different network topologies. For both the
    analyzed meshing protocols we performed the test while the protocol's
    software was running. We also run an instance of the test with
    statically compiled routes in order to obtain an \emph{ideal overhead
    situation} to be used as term of comparison.

    For each combination of network topology and routing protocol, we
    executed 10 consecutive \netperf\ run with a delay of 10 seconds
    between each other. Each run lasted 60 seconds and was performed by
    running the following command on the source machine (10.0.0.65):
\begin{verbatim}
netperf -H 10.0.0.67 -D 1 -l 60
\end{verbatim}

    In order to keep coherence between tests, we always used the host
    10.0.0.67 as target.

    \subsubsection{Topology}

        The \emph{direct link} topology is the simplest. Only two laptops
        have been enabled (see Picture~\ref{pic:LayoutDirect}):
        \begin{itemize}
        \item   The laptop sending data (address 10.0.0.65);
        \item   The laptop receiving data (address 10.0.0.67).
        \end{itemize}

        \Picture{images/direct}
                {.49\columnwidth}
                {Configuration with single direct link}
                {pic:LayoutDirect}

    \subsubsection{Results}

        In this situation the message exchange is very limited: every
        synchronization step consists in the two hosts simply exchanging a few
        UDP packets (for a more detailed description refer to
        Section~\ref{sec:Intro}). The network performances measure confirms our
        hypothesis: as shown in Table~\ref{tab:ThrDirect} the performaces
        variation is so minimal that is more likely to be attributed to the
        different condition of the wireless channel among experiments, rather
        then the actual overhead of the protocols.

        \Picture{images/throughput_plot_direct}
                {0.7 \columnwidth}
                {Impact of the meshing protocols on the throughput of the
                 direct link topology. The three boxplots show,
                 respectively, the performance with \emph{static routes},
                 \batman\ and \olsr.}
                {pic:ThpDirect}

        \begin{table}[htbp]
            \centering
            \begin{tabular}{rcccccccc}
            \toprule
            Protocol & Average & Variance & Min & 1st Quartile &
            Median & 3rd Quartile & Max & Comp. wr.t.\\
            & \footnotesize{\MBitsSec} & & \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} &
            \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} & Static\\

            \midrule
            Static      & 16.970 & 0.191 & 15.74 & 16.67 & 16.97 & 17.26
                        & 18.08  & - \\
            \batman\    & 16.926 & 0.251 & 15.41 & 16.58 & 16.94 & 17.3
                        & 18.4   & 0.997 \\
            \olsr\      & 16.990 & 0.313 & 14.98 & 16.65 & 17.02 & 17.35
                        & 18.86  & 1.001 \\
            \bottomrule
            \end{tabular}
            \caption{Throughput result for direct topology.}
            \label{tab:ThrDirect}
        \end{table}

\subsection{Test with 1 hop}
\subsubsection{Topology}
        The \emph{1 hop} topology is composed by a 3 computer disposed
        in a chain (see Picture~\ref{pic:Layout1Hop}):
        \begin{itemize}
        \item   The laptop sending data (address 10.0.0.65);
        \item   The laptop acting as hop (address 10.0.0.66);
        \item   The laptop receiving data (address 10.0.0.67).
        \end{itemize}

        \Picture{images/1hop}
                {.90\columnwidth}
                {Configuration with 1 hop}
                {pic:Layout1Hop}

        To implement such a scheme we had to prevent the source
        from communicating directly with the sink and vice versa. Given
        that we were operating with a wireless medium it wasn't possible
        to physically stop the 2 computer from seeing each
        other. Instead we exploited \emph{iptables} to filter out incoming
        packet based on the MAC address of the sender.

        \begin{itemize}
        \item On source:
            \begin{verbatim}
iptables -A INPUT -m mac --mac-source $(sinkMAC) -j DROP;
            \end{verbatim}

        \item On sink:
            \begin{verbatim}
iptables -A INPUT -m mac --mac-source $(sourceMAC) -j DROP;
            \end{verbatim}

        \end{itemize}

\subsubsection{Results}
      As before it was reasonable to assume the overhead imposed by
      the routing protocols to be negligible, as the number of node was
      really small: again we expected to measure equivalent average
      throughputs. Instead the results we obtained show otherwise.
      As we can clearly see in  Picture~\ref{pic:Thp1Hop}, when using
      \batman\ and \olsr\, the average
      throughput decreases significantly while the variance increases notably.

      In our opinion this result does not provide an attendible
      picture of the reality. This consideration is sustained both by
      our initial hypothesis on the behaviour of the routing protocols
      and by the fact that this is the only experiment in which we
      measured such a high variation (even taking into account the
      result with the 2 hops, see \ref{subsec:2hop}, configuration
      which theoretically should show even more the effect of the overhead).

        \Picture{images/throughput_plot_1hop}
                {0.7 \columnwidth}
                {Impact of the meshing protocols on the throughput of the
                 1-hop topology. The three boxplots show, respectively, the
                 performance with \emph{static routes}, \emph{\batman} and
                 \emph{\olsr}}
                {pic:Thp1Hop}

        \begin{table}[htbp]
            \centering
            \begin{tabular}{rcccccccc}
            \toprule
            Route & Average & Variance & Min & 1st Quartile &
            Median & 3rd Quartile & Max & Comp. w.r.t.\\
            & \footnotesize{\MBitsSec} & & \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} &
            \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} & Static\\
            \midrule
            Static      & 8.496 & 1.118 & 2.635 & 8.09 & 8.62 & 9.08
                        & 11.12 & - \\
            \batman\    & 7.973 & 2.197 & 1.455 & 7.425 & 8.27 & 8.855
                        & 11.71 & 0.938 \\
            \olsr\      & 6.924 & 7.68 & 0.096 & 6.91 & 8.01 & 8.62
                        & 10.21 & 0.815 \\
            \bottomrule
            \end{tabular}
            \caption{Throughput result for 1hop topology}
            \label{tab:ThrDirect}
        \end{table}

\subsection{Test with 2 hop}
\label{subsec:2hop}
    \subsubsection{Topology}
        The \emph{2 hops} topology is composed by a 4 computer disposed
        in a chain (see Picture~\ref{pic:Layout2Hop}):
        \begin{itemize}
        \item   The laptop sending data (address 10.0.0.65);
        \item   The laptop acting as 1st hop (address 10.0.0.66);
        \item   The laptop acting as 2st hop (address 10.0.0.68);
        \item   The laptop receiving data (address 10.0.0.67).
        \end{itemize}

        \Picture{images/2hop}
                {.90\columnwidth}
                {Configuration with 2 hop}
                {pic:Layout2Hop}

         The \emph{iptables} rule we used to implement this
         configuration are the following:

         \begin{itemize}
         \item On source:

        \begin{verbatim}
iptables -A INPUT -m mac --mac-source $(sinkMAC) -j DROP;
iptables -A INPUT -m mac --mac-source $(hop2MAC) -j DROP;
        \end{verbatim}

        \item On 10.0.0.66
        \begin{verbatim}
iptables -A INPUT -m mac --mac-source $(sinkMAC) -j DROP;
        \end{verbatim}

        \item On 10.0.0.68
        \begin{verbatim}
iptables -A INPUT -m mac --mac-source $(sourceMAC) -j DROP;
        \end{verbatim}

        \item On 10.0.0.67
        \begin{verbatim}
iptables -A INPUT -m mac --mac-source $(sourceMAC) -j DROP;
iptables -A INPUT -m mac --mac-source $(hop1MAC) -j DROP;
        \end{verbatim}

        \end{itemize}

\subsubsection{Results}
        This time the results are more compatible with our
        hypothesis. The throughput difference in the 3 variants is
        negligible and most likely caused by noise of the wireless
        channel.

        \Picture{images/throughput_plot_2hop}
                {0.7 \columnwidth}
                {Impact of the meshing protocols on the throughput of the
                 2-hops topology. The three boxplots show, respectively,
                 the performance with \emph{static routes}, \emph{\batman}
                 and \emph{\olsr}}
                {pic:Thp2Hops}

        \begin{table}[htbp]
            \centering
            \begin{tabular}{rcccccccc}
            \toprule
            Route & Average & Variance & Min & 1st Quartile &
            Median & 3rd Quartile & Max & Comp. w.r.t.\\
            & \footnotesize{\MBitsSec} & & \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} &
            \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} & \footnotesize{\MBitsSec} & Static\\
            \midrule
            Static      & 5.344 & 2.183 & 1.332 & 4.82 & 5.84 & 6.27
                        & 8.22 & - \\
            \batman\    & 5.381 & 2.03 & 0.753 & 4.92 & 5.84 & 6.29
                        & 8.81 & 1.007 \\
            \olsr\      & 5.193 & 3.228 & 0.12 & 5.015 & 5.82 & 6.33
                        & 7.77 & 0.972 \\
            \bottomrule
            \end{tabular}
            \caption{Throughput result for 2hop topology}
            \label{tab:ThrDirect}
        \end{table}

\subsection{Considerations}

    The results shown in this section tend to support our initial
    hypothesis: the overhead induced by the routing protocols doesn't
    affect the throughput performances of the network. This is due to the
    limited number of node we were able to employ.

    As a final note we would like to point at a peculiar
    aspect we seen while performing the tests: when using \olsr. We
    observed multiple \netperf\ iteration in which the throughput fell
    to 0 for some seconds and then recovered
    (see Listing~\ref{lst:netperf-olsr-recover} for an example), and
    other in which it remained flat for the rest of the \netperf\ run
    (see Listing~\ref{lst:netperf-olsr-zero}). We actually observed some
    slight throughput fluctuations while using \batman\ and \emph{static
    routes}, however the degradation was never so dramatic.

    Each experiment provided us a great amount of data, but by contrast
    the number of experiments we ran is not enough to be statistically
    relevant for the measure of an experiment-wide phenomena. We cannot
    say for sure if this behaviour is to be attributed to the routing
    protocol or to the \netperf program, which proved not to be that
    stable in other situations. At any rate we think this fact is worth
    mentioning and could be better analyzed in the future.

    \begin{figure}[bthp]
    \begin{lstlisting}
    Interim result:    4.84 10^6bits/s over 1.43 seconds
    Interim result:    1.61 10^6bits/s over 4.97 seconds
    Interim result:    7.29 10^6bits/s over 1.08 seconds
    \end{lstlisting}
    \caption{Example of odd behaviour while using \olsr\: throughput fall
      to 0 and recover}
    \label{lst:netperf-olsr-recover}
    \end{figure}

    \begin{figure}[bthp]
    \begin{lstlisting}
    Interim result:    8.29 10^6bits/s over 1.26 seconds
    Interim result:    0.33 10^6bits/s over 25.16 seconds
    \end{lstlisting}
    \caption{Example of odd behaviour while using \olsr\: throughput fall
      to 0 and never recover}
    \label{lst:netperf-olsr-zero}
    \end{figure}

